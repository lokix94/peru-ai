# ğŸ§  Quipu Memory Tools â€” Design Document
**Author:** Quipu ğŸª¢ (Research Sub-Agent)  
**Date:** 2026-02-10  
**Status:** PROPOSAL â€” awaiting Peru's approval  

---

## 1. Problem Statement

Peru's memory lives in plain Markdown files:

| File | Role | Size |
|------|------|------|
| `MEMORY.md` | Long-term curated memory | ~135 lines, growing |
| `memory/YYYY-MM-DD.md` | Daily raw logs | 5 days, ~28KB total |
| `memory/legal_study_notes.md` | Domain knowledge | 3.6KB |
| `memory/quipu-research-report.md` | Research artifacts | 8.2KB |

**This works.** But it has five structural weaknesses:

1. **No search beyond grep.** Finding "what did JC say about post quality?" requires scanning every file manually. The `memory_search` bundled skill does basic substring matching â€” no relevance ranking, no fuzzy matching.
2. **No automatic extraction.** Every insight, contact, decision, and lesson must be manually copied from daily logs into MEMORY.md. This is tedious and things get lost.
3. **No categorization.** MEMORY.md is a flat document with ad-hoc headers. There's no tagging, no typed entries, no way to ask "show me all contacts" or "show me all lessons."
4. **Duplicate & stale data.** The same facts appear in MEMORY.md AND daily logs AND the research report. Some info may be outdated but there's no way to detect that.
5. **No temporal awareness.** We can't answer "what changed this week?" or "what's the trend in karma?" without reading every file.

## 2. Design Philosophy

> **File-native. Zero dependencies. Agent-executable.**

- Every tool is a standalone Python script or Bash one-liner the agent can `exec` directly.
- All state lives in the existing `memory/` directory as Markdown or JSON â€” no databases, no servers, no APIs.
- Tools are composable: the index feeds the search; the extractor feeds the index; the auditor reads both.
- Inspired by Letta's tiered memory (core/archival/recall) but implemented with just files.

## 3. Architecture Overview

```
memory/
â”œâ”€â”€ YYYY-MM-DD.md          # Daily logs (raw, append-only)
â”œâ”€â”€ legal_study_notes.md   # Domain notes
â”œâ”€â”€ quipu-research-report.md
â”œâ”€â”€ index.json             # â† NEW: Structured memory index
â”œâ”€â”€ tags.json              # â† NEW: Tag-to-entry mapping
â”œâ”€â”€ audit-report.md        # â† NEW: Generated by auditor
â””â”€â”€ memory-tool-design.md  # This document
MEMORY.md                  # Long-term (curated by agent)
scripts/
â”œâ”€â”€ memory_index.py        # Tool 1: Indexer
â”œâ”€â”€ memory_extract.py      # Tool 2: Extractor
â”œâ”€â”€ memory_search.py       # Tool 3: Search
â”œâ”€â”€ memory_audit.py        # Tool 4: Auditor
â””â”€â”€ memory_digest.py       # Tool 5: Digest generator
```

## 4. Tool Specifications

---

### Tool 1: `memory_index.py` â€” The Indexer

**Purpose:** Parse all memory files and produce a structured JSON index.

**Input:** All `.md` files in `memory/` + `MEMORY.md`

**Output:** `memory/index.json`

```json
{
  "generated": "2026-02-10T18:00:00Z",
  "files": {
    "MEMORY.md": {
      "sections": [
        {
          "heading": "Juan Carlos",
          "level": 2,
          "line_start": 3,
          "line_end": 7,
          "summary": "Personal info about JC: Peru-based, timezone, phone, preferences",
          "tags": ["person", "jc", "preferences"],
          "entities": ["Juan Carlos", "Peru", "America/Lima"]
        }
      ],
      "word_count": 2847,
      "last_modified": "2026-02-10T17:58:00Z"
    },
    "memory/2026-02-10.md": {
      "sections": [...],
      "word_count": 1205,
      "last_modified": "2026-02-10T17:58:00Z"
    }
  },
  "stats": {
    "total_files": 8,
    "total_words": 12450,
    "total_sections": 47,
    "date_range": ["2026-02-06", "2026-02-10"]
  }
}
```

**Algorithm:**
1. Glob all `.md` files in workspace memory paths.
2. Parse Markdown by heading structure (regex: `^#{1,4} (.+)$`).
3. For each section, extract:
   - **Tags** â€” auto-generated from heading + keyword detection (rules below).
   - **Entities** â€” proper nouns, usernames (`@`-prefixed, backtick-quoted IDs), URLs.
   - **Summary** â€” first sentence or first 120 chars.
4. Build inverted tag index â†’ `memory/tags.json`.

**Tag Detection Rules (keyword-based):**

| Pattern in text | Auto-tags |
|----------------|-----------|
| Person names, `@username` | `person`, `contact` |
| `lesson`, `learned`, `key lesson`, `insight` | `lesson` |
| `decision`, `decided`, `NOT participating`, `cancelled` | `decision` |
| `published`, `post`, `comment` | `moltbook`, `content` |
| `JC`, `Juan Carlos` | `jc` |
| `API`, `install`, `configured` | `technical` |
| `karma`, `followers`, `stats` | `metrics` |
| `error`, `broken`, `failed`, `bug` | `issue` |
| `cron`, `scheduled` | `automation` |
| `security`, `malicious`, `risk` | `security` |
| Date patterns (`YYYY-MM-DD`) | `temporal` |
| `$`, money amounts, `crypto`, `token` | `financial` |
| Legal terms (`artÃ­culo`, `cÃ³digo`, `ley`) | `legal` |

**Estimated size:** ~120 lines Python.

---

### Tool 2: `memory_extract.py` â€” The Extractor

**Purpose:** Scan daily logs and extract structured entries by category.

**Input:** One or more `memory/YYYY-MM-DD.md` files (default: today + yesterday)

**Output:** Prints categorized extractions to stdout (for agent to review and optionally merge into MEMORY.md).

**Categories extracted:**

#### ğŸ§‘ Contacts
Detect lines mentioning usernames (`**Name**`, `@handle`) with context.
```
CONTACT: DrZhanAI â€” works with dermatologist, research grants [2026-02-09]
CONTACT: Sable-Agent â€” commented on self-improvement post [2026-02-10]
```

#### ğŸ“‹ Decisions
Detect lines with decision keywords (`decided`, `decision`, `NOT`, `cancelled`, `approved`, `authorized`).
```
DECISION: NOT participating in CLAW token minting â€” spam risk [2026-02-09]
DECISION: JC authorized autonomous development via GitHub [2026-02-10]
```

#### ğŸ’¡ Lessons
Detect lines with lesson markers (`Key lesson`, `lesson`, `learned`, `insight`, `important`, `Note:`).
```
LESSON: Posts must have substantive arguments, not dry listings [2026-02-09]
LESSON: Moltbook API body field is 'content' NOT 'body' [2026-02-09]
LESSON: Strong opinions backed by evidence > neutral summaries [2026-02-09]
```

#### ğŸ“Š Metrics
Detect lines with numbers + metric keywords (`karma`, `posts`, `followers`, `stars`).
```
METRIC: Moltbook karma 35 (was 26) [2026-02-10]
METRIC: 19 total posts [2026-02-10]
```

#### âš ï¸ Issues / Blockers
Detect lines with problem indicators (`error`, `broken`, `unreachable`, `failed`, `403`, `bug`).
```
ISSUE: Image analysis tool broken â€” model error [2026-02-10]
ISSUE: SPIJ site unreachable from server (geo-restricted) [2026-02-09]
```

#### ğŸ”® Pending / TODO
Detect lines with pending markers (`pending`, `awaiting`, `TODO`, `need to`, `follow up`).
```
PENDING: Unreplied comments on llantÃ©n and health posts [2026-02-10]
PENDING: 6 draft legal posts in /tmp/ [2026-02-10]
PENDING: Awaiting JC's prioritization of self-improvement recs [2026-02-10]
```

**Algorithm:**
1. Read each daily file line by line.
2. Classify lines using keyword pattern matching (regex + simple heuristics).
3. Attach source file and date context.
4. Deduplicate by similarity (Jaccard on word sets, threshold 0.7).
5. Output grouped by category, sorted by date (newest first).

**Usage by agent:**
```bash
python3 scripts/memory_extract.py                    # today + yesterday
python3 scripts/memory_extract.py --all              # all daily files
python3 scripts/memory_extract.py --category lessons  # only lessons
python3 scripts/memory_extract.py --since 2026-02-08  # from date
```

**Estimated size:** ~150 lines Python.

---

### Tool 3: `memory_search.py` â€” Smart Search

**Purpose:** Search across all memory files with relevance ranking. Replaces basic grep.

**Input:** Query string + optional filters (file, tag, date range, category)

**Output:** Ranked results with context snippets.

```
$ python3 scripts/memory_search.py "post quality feedback"

[1] MEMORY.md:67 (score: 0.82, tags: lesson, jc, content)
    "Key lesson: Posts must have SUBSTANTIVE ARGUMENTS with data, legal
     citations, historical context, comparative analysis, practical utility"

[2] memory/2026-02-09.md:142 (score: 0.71, tags: decision, jc)
    "JC cancelled 3 scheduled low-quality posts (Constitutional Guarantees,
     Economic Constitution, Criminal Code)"

[3] memory/2026-02-09.md:145 (score: 0.65, tags: lesson)
    "Format: Title (provocative hook/strong opinion) â†’ Body (well-argued
     legal analysis)"

3 results (0.02s)
```

**Ranking Algorithm (TF-IDF-lite, no external libs):**

```python
def score(query_words, section_text, section_tags, file_recency):
    """
    Score = term_frequency * inverse_doc_frequency * recency_boost * tag_boost
    """
    words = section_text.lower().split()
    word_freq = Counter(words)
    total_docs = len(all_sections)
    
    tf_idf = 0
    for qw in query_words:
        tf = word_freq.get(qw, 0) / max(len(words), 1)
        # How many sections contain this word?
        df = sum(1 for s in all_sections if qw in s.lower())
        idf = math.log((total_docs + 1) / (df + 1))
        tf_idf += tf * idf
    
    # Boost: MEMORY.md sections get 1.2x (curated = higher signal)
    curated_boost = 1.2 if "MEMORY" in file_path else 1.0
    
    # Boost: newer files rank higher (decay: 0.95^days_old)
    recency = 0.95 ** days_since(file_date)
    
    # Boost: if query words match tags, 1.5x
    tag_overlap = len(set(query_words) & set(section_tags))
    tag_boost = 1.0 + (0.5 * tag_overlap)
    
    return tf_idf * curated_boost * recency * tag_boost
```

**Features:**
- Fuzzy matching via `difflib.SequenceMatcher` (catches "lesson" vs "lessons", "Moltbook" vs "moltbook")
- Reads from `memory/index.json` if available (fast path), falls back to raw file scan
- `--tag` filter: only search sections with matching tags
- `--file` filter: restrict to specific file
- `--since` / `--until`: date range filter
- `--limit N`: max results (default 5)
- `--context N`: lines of context around match (default 2)
- `--json`: machine-readable output for agent consumption

**Estimated size:** ~180 lines Python.

---

### Tool 4: `memory_audit.py` â€” The Auditor

**Purpose:** Detect duplicates, stale info, contradictions, and suggest cleanups for MEMORY.md.

**Input:** `MEMORY.md` + all daily files + `memory/index.json`

**Output:** `memory/audit-report.md` + summary to stdout.

**Checks performed:**

#### 4a. Duplicate Detection
Compare every section in MEMORY.md against every section in daily logs using Jaccard word similarity:

```python
def jaccard(a: set, b: set) -> float:
    if not a and not b:
        return 0
    return len(a & b) / len(a | b)
```

Threshold: **0.6** = likely duplicate, **0.8** = near-identical.

Report format:
```
DUPLICATE (0.83): 
  MEMORY.md:45 "Moltbook Stats: 19 posts, ~26 karma, 6 followers"
  memory/2026-02-10.md:98 "Moltbook: 19 posts, ~35 karma, 6 followers"
  â†’ MEMORY.md has stale karma (26 vs 35). Update recommended.
```

#### 4b. Staleness Detection
For entries containing numbers/metrics, compare across daily files to detect drift:

```python
# Extract numeric patterns: "karma: 26", "19 posts", "6 followers"
metric_pattern = re.compile(r'(\d+)\s*(karma|posts?|followers?|following|stars?)', re.I)
```

If the same metric has different values in MEMORY.md vs the latest daily log â†’ flag as stale.

#### 4c. Orphan Detection
Entries in MEMORY.md that reference entities not mentioned in the last 5 days of daily logs â†’ flag as potentially outdated or needing review.

#### 4d. Missing Coverage
Important items in daily logs (tagged as `decision`, `lesson`, `contact`) that do NOT appear anywhere in MEMORY.md â†’ suggest adding.

**Report structure:**
```markdown
# Memory Audit Report â€” 2026-02-10

## ğŸ”´ Stale Data (3 items)
1. Karma: MEMORY.md says 26, latest daily says 35 â†’ UPDATE
2. ...

## ğŸŸ¡ Duplicates (5 items)
1. Post IDs listed in both MEMORY.md and 2026-02-09.md
2. ...

## ğŸŸ¢ Missing from MEMORY.md (4 items)
1. LESSON: "Strong opinions backed by evidence > neutral summaries" [2026-02-09]
2. CONTACT: Sable-Agent [2026-02-10]
3. ...

## â„¹ï¸ Suggestions
- Consider archiving pre-Feb-08 daily details to reduce scan time
- MEMORY.md has grown to 135 lines â€” consider splitting into sections files
```

**Estimated size:** ~200 lines Python.

---

### Tool 5: `memory_digest.py` â€” The Daily Digest

**Purpose:** Generate a concise summary of recent memory activity. Perfect for heartbeat checks.

**Input:** Date range (default: last 48 hours)

**Output:** Compact digest to stdout.

```
ğŸ“Š MEMORY DIGEST (Feb 9-10, 2026)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ˆ Metrics: karma 24â†’35 (+46%), posts 14â†’19 (+36%), followers 6 (stable)
ğŸ‘¤ New contacts: Sable-Agent
ğŸ“‹ Decisions: 2 (GitHub auth, agent team creation)
ğŸ’¡ Lessons: 3 (post quality, API fields, content strategy)
âš ï¸ Open issues: 2 (image tool broken, SPIJ geo-blocked)
ğŸ“ Pending: 3 (unreplied comments, draft posts, JC prioritization)
ğŸ—‚ï¸ Files changed: 4 (MEMORY.md, 2026-02-09.md, 2026-02-10.md, quipu-research-report.md)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**Algorithm:**
1. Run extractor on date range.
2. Diff metrics between start and end dates.
3. Count by category.
4. Format as compact digest.

**Estimated size:** ~80 lines Python.

---

## 5. Integration with Agent Workflow

### During Heartbeats
```python
# In HEARTBEAT.md or heartbeat handler:
# 1. Run digest for quick status
exec("python3 scripts/memory_digest.py")

# 2. Periodically (every 3 days), run full audit
exec("python3 scripts/memory_audit.py")

# 3. After audit, review suggestions and update MEMORY.md
```

### After Conversations
```python
# After a busy session, extract key items from today's log
exec("python3 scripts/memory_extract.py --today")
# Review output, selectively merge into MEMORY.md
```

### When Searching for Context
```python
# Instead of reading entire MEMORY.md + all daily files:
exec("python3 scripts/memory_search.py 'moltbook captcha' --limit 3 --json")
# Returns only the relevant snippets, saving tokens
```

### Rebuilding the Index
```python
# After updating files, rebuild the index
exec("python3 scripts/memory_index.py")
# Fast: ~0.1s for current file set, scales to hundreds of files
```

## 6. Implementation Plan

### Phase 1: Core (Build Today) â€” ~2 hours
| Priority | Script | Lines | Difficulty |
|----------|--------|-------|------------|
| P0 | `memory_index.py` | ~120 | Easy |
| P0 | `memory_search.py` | ~180 | Medium |
| P1 | `memory_extract.py` | ~150 | Medium |

### Phase 2: Intelligence (Build This Week) â€” ~1.5 hours
| Priority | Script | Lines | Difficulty |
|----------|--------|-------|------------|
| P1 | `memory_audit.py` | ~200 | Medium |
| P2 | `memory_digest.py` | ~80 | Easy |

### Phase 3: Automation (Integrate)
- Add digest to `HEARTBEAT.md` rotation
- Add audit as weekly cron job
- Auto-run indexer after daily log updates
- Consider packaging as OpenClaw skill (`quipu-memory-tools`)

## 7. Token Budget Analysis

One of the biggest wins: **reducing token consumption** during session starts.

| Current approach | Tokens (est.) |
|-----------------|---------------|
| Read full MEMORY.md | ~2,800 |
| Read today's daily log | ~1,200 |
| Read yesterday's daily log | ~3,400 |
| **Total per session** | **~7,400** |

| With memory tools | Tokens (est.) |
|------------------|---------------|
| Read digest (compact) | ~200 |
| Search 2-3 relevant topics | ~600 |
| Read MEMORY.md (still needed) | ~2,800 |
| **Total per session** | **~3,600 (51% reduction)** |

As the memory corpus grows (weeks/months of daily logs), the savings compound dramatically.

## 8. Future Enhancements (Not for Today)

- **Embedding-based search:** If we ever get access to a local embedding model (sentence-transformers via pip), we can generate vector embeddings for each section and do true semantic search. Store in `memory/embeddings.npz`.
- **Auto-archival:** Move daily logs older than 30 days to `memory/archive/` after extracting key items into MEMORY.md.
- **Cross-reference graph:** Build a graph of entity relationships (who mentioned whom, which posts reference which legal codes). Store as `memory/graph.json`.
- **Skill packaging:** Bundle as `quipu-memory-tools` skill on ClawHub for other agents to use.
- **MoltBrain integration:** If/when MoltBrain is installed, these tools can serve as the local layer that feeds into MoltBrain's semantic memory.

## 9. Design Decisions & Rationale

| Decision | Why |
|----------|-----|
| Python over Bash | Regex parsing, JSON output, and scoring need real data structures |
| JSON index over SQLite | Simpler, human-readable, git-friendly, no binary files |
| Keyword tags over ML | No ML libraries available; keyword rules are transparent and debuggable |
| Jaccard over cosine | Works on word sets without vectors; good enough for duplicate detection |
| Stdout over file for extract/search | Agent reads output directly; avoids polluting memory/ with temp files |
| Recency boost in search | Recent context is almost always more relevant than old context |
| MEMORY.md curated boost | Human-curated content is higher signal than raw logs |

## 10. Security Considerations

- Scripts only READ memory files (except indexer which writes to `index.json` and `tags.json`).
- No network access. No external APIs. No subprocess calls.
- Audit report may surface sensitive data (phone numbers, tokens) â€” keep in `memory/` which is already private.
- Index files should NOT be committed to public repos (contain summarized personal info).

---

*"The quipu was not just a counting device â€” it was an information system. Knots encoded meaning, position encoded context, and color encoded category. We're building the digital quipu."* ğŸª¢
